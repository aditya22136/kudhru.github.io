---
layout: post
title: Generative Adversarial Imitation Learning
author: Jiaming Song
tags:
- reading
---

## Generative Adversarial Imitation Learning

### Inverse Reinforcement Learning

Maximum casual entropy IRL takes the form of


$$
\max_{c \in C} (\min_{\pi \in \Pi} - H(\pi) + \mathbb{E}_{\pi}[c(s, a)]) - \mathbb{E}_{\pi_{E}}[c(s, a)]
$$


where $$H(\pi) = \mathbb{E}_\pi [-\log \pi(a \lvert s)]$$ is the $$\gamma$$ discounted casual entropy of the policy $$\pi$$. Maximum casual entropy IRL looks for a cost function $$c \in \mathcal{C}$$ that assigns low cost to the expert policy and high cost to other policies, thereby allowing the expert policy to be found via a certain reinforcement learning procedure.


$$
\mathrm{RL}(c) = \arg \min_{\pi \in \Pi} -H(\pi) + \mathbb{E}_\pi [c(s, a)]
$$


which maps a cost function to high-entropy policies that minimize the expected cumulative cost.



### Charaterizing the Induced Optimal Policy

To begin our search for an algorithm that both bypasses an intermedia te IRL step, and is suitable for large environments, we will study policies found by reinforcement learning on costs learned by IRL on $$\mathcal{C}$$.

To prevent overfitting, a (closed, proper) convex cost function regularizer $$\psi: \mathbb{R}^{\mathcal{S} \times \mathcal{A}} \rightarrow \mathbb{R}$$ into the study.

The IRL primitive procedure is defined as:


$$
\mathrm{IRL}_\psi(\pi_E) = \arg \max_{c\in\mathbb{R}^{\mathcal{S} \times \mathcal{A}}} -\psi(c) + (\min_{\pi \in \Pi} - H(\pi) + \mathbb{E}_{\pi}[c(s, a)]) - \mathbb{E}_{\pi_{E}}[c(s, a)]
$$

### Generative Adversarial Imitation Learning

One selection for a regularzier would be 


$$
\psi_{GA}(c) =
  \begin{cases}
    \mathbb{E}_{\pi_E} [g(c(s, a))]       & \quad \text{if } c < 0 \\
    +\infty  & \quad \text{otherwise}                                       \\
  \end{cases}
$$


where


$$
g(x) = 
\begin{cases}
-x -\log(1 - e^x) & \quad \text{if } x < 0 \\
+\infty & \quad \text{otherwise}
\end{cases}
$$


This regularizer places low penalty on cost functions $$c$$ that assign an amount of negative cost to expert state-action pairs; if $$c$$, however, assigns large costs to the expert, then $$\psi_{GA}$$ will heavily penalize $$c$$.

The algorithm called *generative adversarial imitation learning*, wishes to find a saddle point $$(\pi, D)$$ of the expression:


$$
\mathbb{E}_{\pi} [\log (D(s, a))] + \mathbb{E}_{\pi_E}[\log(1 - D(s, a))] - \lambda H(\pi)
$$


To do so, the algorithm alternate between an Adam step and a TRPO step.

The Adam step updates with the gradient


$$
\mathbb{E}_{\tau_i}[\nabla_w \log (D_w(s, a))] + \mathbb{E}_{\tau_E}[\nabla_w \log (1 - D_w(s, a))]
$$


The TRPO updates the KL-constrained natural gradient step with


$$
\mathbb{E}_{\tau_i}[\nabla_\theta \log \pi_\theta (a \lvert s) Q(s, a)] - \lambda \nabla_\theta H(\pi_\theta), \\
\text{where } Q(\bar{s}, \bar{a}) = \mathbb{E}_{\tau_i}[\log (D_{w_{i+1}} (s, a) \lvert s_0 = \bar{s}, a_0 = \bar{a})]
$$
