---
layout: post
title: Data Generation as Sequential Decision Making
author: Jiaming Song
tags:
- reading
---

## Data Generation as Sequential Decision Making

> Bachman and Precup. [https://arxiv.org/pdf/1506.03504v3.pdf](https://arxiv.org/pdf/1506.03504v3.pdf)

The observation is that many generative models can be described as sequential decision making, whereas the lower bound objective of variational auto-encoders are analogous to **generalized guided policy search**.

### Generalized Guided Policy Search

The expanded version of guided policy search includes any optimization of the general form:


$$
\min_{p, q} \mathbb{E}_{i_q \sim \mathcal{I}_q} \mathbb{E}_{i_p \sim \mathcal{I}_p(\cdot \lvert i_q)} [\mathbb{E}_{\tau \sim q(\tau \lvert i_q, i_p)}[\ell(\tau, i_q, i_p)] + \lambda \cdot \mathrm{div}(q(\tau\lvert i_q, i_p), p(\tau \lvert i_p))]
$$


where $$p$$ indicates the primary policy, $$q$$ indicates the guide policy, $$\mathcal{I}_q$$ indicates a distribution over information available only to $$q$$, and $$\mathcal{I}_p$$ indicates a distribution over information available to both $$p$$ and $$q$$.

$$\ell(\tau, i_q, i_p)$$ computes the cost of trajectory $$\tau$$ in the context of $$i_q/i_p$$, and $$\mathrm{div}$$ is a dissimilarity measure.



### Deep AutoRegressive Models

$$p(x)$$ and $$q(x)$$ have the following forms:


$$
p(x) = \sum_{z} p(x\lvert z) p(z), \quad \mathrm{with} \quad p(z) = p_0 (z_0) \prod_{t=1}^{T} p_t(z_t \lvert z_0, \ldots, z_{t-1}) \\
q(\tau \lvert x^\star) = q(x \lvert z_0, \ldots, z_T, x^\star) q_0(z_0 \lvert x^\star) \prod_{t=1}^{T} q_t (z_t \lvert z_0, \dots, z_{t-1}, x^\star)
$$
$$q$$ provides a guide policy over the trajectories $$\tau = \{z_0, \ldots, z_T, x^\star\}$$



### Time-Reversible Stochastic Process

$$p(x_T)$$ and $$q(x_0)$$ has the following form:


$$
p(x_T) = \sum_{x_0, \ldots, x_{T-1}} p_{T}(x_T \lvert x_{T-1})p_0(x_0) \prod_{t=1}^{T-1} p_t(x_t \lvert x_{t-1}) \\
q_0(x_0) = \sum_{x_1, \ldots, x_{T}} q_1(x_0 \lvert x_1) \mathcal{D}_{\mathcal{X}}(x_T) \prod_{t=2}^{T} q_t (x_{t-1} \lvert x_t) \approx p_0(x_0)
$$


$$q$$ provides a guide policy over the trajectories $$\tau = \{x_0, \ldots, x_T \}$$.



### Learning Generative Stochastic Processes with LSTMs (DRAW)

$$p(\tau)$$ and $$q(\tau)$$ are defined as:


$$
p(\tau) = p(x \lvert s_\theta (\tau_{<x}))p_0(z_0) \prod_{t=1}^{T} p_t (z_t), \quad \mathrm{with} \quad \tau_{<x} = \{z_0, \ldots, z_T\} \\
q(\tau \lvert x^\star) = q(x \lvert s_\phi(\tau_{<x}), x^\star) q_0(z_0 \lvert x^\star) \prod_{t=1}^{T} q_t(z_t \lvert \tilde{s}_t, x^\star)
$$


where $$s_\phi(\tau_{<x})$$ indicates a state trajectory $$\{\tilde{s}_0, \ldots,  \tilde{s}_t\}$$ computed recursively from $$\tau_{<x}$$ using the state update $$\tilde{s}_t \leftarrow f_\phi(\tilde{s}_{t-1}, g_\phi(s_\theta(\tau_{<t}, x^\star)))$$.



### Developing Models for Sequential Imputation

The objective for imputation is


$$
\min_p \mathbb{E}_{x\sim \mathcal{D}_\mathcal{X}} \mathbb{E}_{m \sim \mathcal{D}_\mathcal{M}} [-\log p(x^u \lvert x^k)]
$$


The trajectory distribution would be $$p(\tau \lvert x^k) = p(z_0 \lvert x^k) \prod_{t=1}^{T} p(z_t \lvert z_0, \ldots, z_{t-1}, x^{k})$$.

We can find an approximately optimal imputation policy by solving:


$$
\min_p \mathbb{E}_{x \sim D_\mathcal{X}}  \mathbb{E}_{m \sim D_\mathcal{M}} \mathbb{E}_{\tau \sim p(\tau \lvert x^k)} [-\log p(x^u \lvert \tau, x^k)]
$$


This is a valid but loose upper bound - a tighten bound can be found by introducing a guide policy. $$p$$ is trained to imitate a guide policy $$q$$ shaped by additional information $$q$$ generates with distribution $$q(\tau \lvert x^u, x^k) = q(z_0 \lvert x^u, x^k) \prod_{t=1}^{T} q(z_t \lvert z_0, \ldots, z_{t-1}, x^u, x^k)$$.

